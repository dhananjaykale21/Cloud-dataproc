

## **ğŸ¯ Lesson Agenda**
1ï¸âƒ£ Introduction to Cloud Dataproc  
2ï¸âƒ£ Why Use Dataproc?  
3ï¸âƒ£ Dataproc Architecture  
4ï¸âƒ£ Setting Up a Dataproc Cluster (Hands-on)  
5ï¸âƒ£ Submitting a Spark Job  
6ï¸âƒ£ Monitoring & Managing Clusters  
7ï¸âƒ£ Cleaning Up Resources  
8ï¸âƒ£ Assignment & Q&A  

---

## **ğŸ“¢ 1ï¸âƒ£ Introduction to Cloud Dataproc** (5 min)  
Start with a simple explanation:  
> **"Cloud Dataproc is a managed service in Google Cloud that helps us run big data workloads using Apache Spark and Hadoop without worrying about infrastructure management."**  

### **ğŸ”¹ Key Features:**  
âœ… **Fast** â€“ Clusters start in less than 90 seconds  
âœ… **Fully Managed** â€“ No need to set up Spark/Hadoop manually  
âœ… **Integrated with GCP** â€“ Works well with **BigQuery, Cloud Storage, etc.**  
âœ… **Cost-Effective** â€“ Pay **only when the cluster is running**  

---

## **ğŸ“¢ 2ï¸âƒ£ Why Use Dataproc? (Real-Life Example)** (5 min)  
ğŸ’¡ **Example:**  
> "Imagine you own a bakery and need to process thousands of customer orders to find the most popular cakes. Instead of manually analyzing the data, you can use Dataproc to quickly run a Spark job and get insights."  

### **Comparison: Manual vs Dataproc**
| Manual Hadoop Setup | Cloud Dataproc |
|---------------------|---------------|
| Takes hours/days | Starts in seconds |
| Manual cluster management | Fully managed |
| Fixed-cost | Pay-as-you-go |

---

## **ğŸ“¢ 3ï¸âƒ£ Dataproc Architecture** (5 min)  
Draw this simple architecture on a **whiteboard** or show a **diagram**:  

ğŸ”¹ **Dataproc Cluster** consists of:  
- ğŸ–¥ï¸ **Master Node** â€“ Controls job execution  
- ğŸ”„ **Worker Nodes** â€“ Process data in parallel  
- ğŸ“¦ **GCS or BigQuery** â€“ Stores input/output data  

---

## **ğŸ“¢ 4ï¸âƒ£ Creating a Dataproc Cluster (Hands-On)** (10 min)  
### **ğŸ› ï¸ Step 1: Enable Dataproc API**  
1. Go to **Google Cloud Console** â†’ **APIs & Services**  
2. Search for **Dataproc API** and enable it  

### **ğŸ› ï¸ Step 2: Create a Dataproc Cluster (Using Console)**  
1. Go to **GCP Console â†’ Dataproc**  
2. Click **"Create Cluster"**  
3. **Enter Cluster Name**: `my-dataproc-cluster`  
4. **Region**: Select a region (e.g., `us-central1`)  
5. **Cluster Type**: Choose **Standard** (2 workers)  
6. **Click "Create"**  

### **ğŸ› ï¸ Step 3: Create a Dataproc Cluster (Using gcloud CLI)**  
Ask students to run this command in **Cloud Shell**:

```sh
gcloud dataproc clusters create my-cluster \
    --region=us-central1 \
    --single-node
```

---

## **ğŸ“¢ 5ï¸âƒ£ Submitting a Spark Job (Hands-On)** (10 min)  
### **Step 1: Create a PySpark Script (`wordcount.py`)**
Ask students to **copy and save** this script:

```python
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder.appName("WordCount").getOrCreate()

# Read a text file from GCS
text_file = spark.read.text("gs://public-datasets-sample/shakespeare.txt")

# Count word occurrences
word_counts = text_file.selectExpr("explode(split(value, ' ')) as word") \
                       .groupby("word") \
                       .count()

# Show top 10 words
word_counts.orderBy("count", ascending=False).show(10)

# Stop Spark session
spark.stop()
```

### **Step 2: Upload Script to Cloud Storage**
```sh
gsutil cp wordcount.py gs://your-bucket-name/
```

### **Step 3: Submit the Job to Dataproc**
```sh
gcloud dataproc jobs submit pyspark gs://your-bucket-name/wordcount.py \
    --cluster=my-cluster --region=us-central1
```

**ğŸ“Œ Explain**:  
- This runs the **word count program** on the **Dataproc cluster**.  
- The results will be printed in **Dataproc Jobs Logs**.  

---

## **ğŸ“¢ 6ï¸âƒ£ Monitoring & Managing Clusters** (5 min)  
### **ğŸ” Check Job Status**
- Go to **Dataproc â†’ Jobs** in the Console  
- Click on the submitted job â†’ View **Logs**  
- If using CLI:
  ```sh
  gcloud dataproc jobs list --region=us-central1
  ```

### **ğŸ” View Cluster Details**
```sh
gcloud dataproc clusters describe my-cluster --region=us-central1
```

---

## **ğŸ“¢ 7ï¸âƒ£ Cleaning Up Resources** (2 min)  
To **save money**, delete the cluster after use:

```sh
gcloud dataproc clusters delete my-cluster --region=us-central1
```

---

## **ğŸ“¢ 8ï¸âƒ£ Assignment for Students**
### **ğŸ¯ Task:**
1. Create a **multi-node** Dataproc cluster.  
2. Upload and run a **different PySpark script** (e.g., sorting words instead of counting).  
3. Submit the job and **analyze the logs**.  

ğŸ“Œ **Bonus Challenge:**  
- Run a **Hive query** on Dataproc.  
- Use **BigQuery** as an output storage.  

---

## **ğŸ¤ Wrap-Up & Q&A**
ğŸ“Œ **Summarize:**  
- Dataproc makes big data processing **easy and fast**.  
- Itâ€™s **fully managed**, cost-effective, and **integrates with GCP**.  
- You can run **Spark, Hadoop, Hive, and more** without managing infrastructure.  

Ask students:  
âœ… **"What was the most interesting part?"**  
âœ… **"Any doubts or challenges faced?"**  

---

## ğŸ **Extra Learning Resources**
1. ğŸ”— [Official Dataproc Documentation](https://cloud.google.com/dataproc/docs)  
2. ğŸ¥ [Google Cloud Dataproc YouTube Playlist](https://www.youtube.com/playlist?list=PLgxF72Xb_7xS77J6yzt3fDBPc6-IJxsci)  
3. ğŸ“– [GCP Free Tier (Try Dataproc for Free)](https://cloud.google.com/free)  

---

### ğŸ¯ **Now You're Ready to Teach Dataproc Like a Pro! ğŸš€**
Would you like **slides** or **a demo script** for the session? ğŸ˜Š
